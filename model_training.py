# -*- coding: utf-8 -*-
"""model_training.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pHfKcQTUL6yP2Bxi8_vo9pUJXWAih48-
"""

import re
import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import OneHotEncoder
import matplotlib.pyplot as plt
import seaborn as sns
#!pip install fuzzywuzzy
from fuzzywuzzy import fuzz
from fuzzywuzzy import process
from scipy.stats import uniform
from math import sqrt
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.model_selection import cross_val_score
from sklearn.linear_model import ElasticNet
from madlan_data_prep import prepare_data
import joblib

excel_file = 'https://github.com/NoaNesher/Advanced-data-mining-in-Python---FinalProject/raw/main/output_all_students_Train_v10.xlsx'
excel_file1 = 'https://github.com/NoaNesher/Advanced-data-mining-in-Python---FinalProject/raw/005898f41b1c7c04ce82d0fcb505ac5bfa0fd4d6/Socioeconomic.xlsx'
df1 = pd.read_excel(excel_file1, engine='openpyxl')
data = pd.read_excel(excel_file, engine='openpyxl')

data1 = df1.copy()
data1= data1.drop(['city_area'], axis=1)
average_data = data1.groupby('City')[['Index_value', 'classification']].mean().reset_index()
data1['City'] = data1['City'].str.replace('יי', 'י')
data1['City'] = data1['City'].str.replace('-', ' ')
data1['City'] = data1['City'].str.replace(r'[^\w\s]', '')
data1['City'] = data1['City'].str.replace(r'\s+', ' ')
average_data = data1.groupby(['City'])[['Index_value', 'classification']].mean().reset_index()

df = prepare_data(data)
new_df_City = pd.merge(df, average_data, on=['City'],how='left')
new_df_City = new_df_City.drop_duplicates()
Nan_data = new_df_City[new_df_City['Index_value'].isnull()]
new_df_City = new_df_City.dropna(subset=['Index_value'])
Nan_data = Nan_data.drop(['Index_value', 'classification'], axis=1)
new_df = pd.DataFrame()
new_df['City'] = Nan_data['City']
new_df = new_df.drop_duplicates()

def find_closest_match(value, choices):
    ratios = process.extract(value, choices, scorer=fuzz.partial_ratio)
    best_match = max(ratios, key=lambda x: x[1])
    return best_match[0]
average_data['Closest_Match'] = average_data['City'].apply(lambda x: find_closest_match(x, new_df['City']))
def check_similarity(str1, str2):
    return fuzz.partial_ratio(str1, str2)

filtered_data = average_data[average_data.apply(lambda row: check_similarity(row['City'], row['Closest_Match']), axis=1) >= 80]
filtered_data = filtered_data.drop(['City'], axis=1)
average_data = average_data.drop(['Closest_Match'], axis=1)
filtered_data = filtered_data.rename(columns={'Closest_Match': 'City'})

avg = pd.concat([filtered_data, average_data], ignore_index=True)
new_df = pd.merge(df, avg, on=['City'],how='left')

condition_replacements = {
    'חדש': 'new',
    'שמור': 'good_condition',
    'משופץ': 'renovated',
    'דורש שיפוץ': 'needs_renovation',
    'לא צויין': 'not_defined',
    'ישן': 'old',
    "None": 'not_defined',
    False: 'not_defined'}

new_df['condition '] = new_df['condition '].replace(condition_replacements)
furniture_replacements = {
    'אין': 'None',
    'מלא': 'Full',
    'לא צויין': 'not_defined',
    'חלקי': 'partial'}

new_df['furniture '] = new_df['furniture '].replace(furniture_replacements)

replacement_values = {
    'בניין': "Apartment",
    'דירה': "Apartment",
    'דירת גן': "Garden_Apartment",
    'דירת גג': "penthouse",
    'פנטהאוז': "penthouse",
    'מיני פנטהאוז': "penthouse",
    "קוטג'": "private",
    'בית פרטי': "private",
    "קוטג' טורי": "private",
    'דו משפחתי': "private",
    'מגרש': "area",
    'נחלה': "area",
    'דופלקס': "duplex",
    'טריפלקס': "duplex",
    'דירת נופש': np.NaN,}

new_df['type'] = new_df['type'].apply(lambda x: replacement_values.get(x, np.NaN))
new_df['type'] = new_df['type'].astype('category')

new_df['floor'] = new_df['floor'].astype(float)
new_df['total_floors'] = new_df['total_floors'].astype(float)

new_df.loc[new_df['type'] == 'penthouse', 'total_floors'] = new_df['floor']

new_df['floor_ratio'] = new_df['floor'] / new_df['total_floors']
mask = new_df['floor'] > new_df['total_floors']
new_df.loc[mask, ['floor', 'total_floors']] = new_df.loc[mask, ['total_floors', 'floor']].values
new_df.loc[new_df['floor'] < 0, 'floor_ratio'] = np.nan
new_df.replace([np.inf, -np.inf], np.nan, inplace=True)
new_df.loc[new_df['type'] == 'private', 'floor_ratio'] = 0
new_df.loc[new_df['type'] == 'area', 'floor_ratio'] = 0
new_df.loc[new_df['type'] == 'Garden_Apartment', 'floor_ratio'] = 0
new_df.loc[new_df['type'] == 'duplex', 'floor_ratio'] = 1

new_df['room_number'] = new_df['room_number'].apply(lambda x: re.findall(r'\d+\.?\d*', str(x)))
new_df['room_number'] = new_df['room_number'].apply(lambda x: float(''.join(x)) if len(x) > 0 else np.nan)
new_df.dropna(subset=['room_number'], inplace=True)

cols_to_drop = ['City', 'Street', 'number_in_street', 'num_of_images',
                'floor_out_of', 'entranceDate ', 'publishedDays ',
                'description ',  'city_area','classification','floor','total_floors']

new_df.drop(cols_to_drop, axis=1, inplace=True)
new_df.dropna(subset=['hasElevator ', 'hasMamad ', 'hasBars ','room_number'], inplace=True)
columns_to_encode = ['type', 'condition ', 'furniture ']
new_df = pd.get_dummies(new_df, columns=columns_to_encode)

y = new_df['price']
X = new_df.drop('price', axis=1)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=None)

from sklearn.impute import SimpleImputer

average_floor_ratio = X_train.loc[X_train['type_Apartment'] == 1, 'floor_ratio'].mean()

X_train.loc[X_train['type_Apartment'] == 1, 'floor_ratio'].fillna(average_floor_ratio, inplace=True)
X_test.loc[X_test['type_Apartment'] == 1, 'floor_ratio'].fillna(average_floor_ratio, inplace=True)

imputer = SimpleImputer(strategy='mean')
X_train[['Area', 'floor_ratio']] = imputer.fit_transform(X_train[['Area', 'floor_ratio']])
X_test[['Area', 'floor_ratio']] = imputer.transform(X_test[['Area', 'floor_ratio']])

X_train['Area'] = pd.to_numeric(X_train['Area'], errors='coerce')
X_train['room_number'] = pd.to_numeric(X_train['room_number'], errors='coerce')
X_train['Area_per_room'] = X_train['Area'] / X_train['room_number']

X_test['Area'] = pd.to_numeric(X_test['Area'], errors='coerce')
X_test['room_number'] = pd.to_numeric(X_test['room_number'], errors='coerce')
X_test['Area_per_room'] = X_test['Area'] / X_test['room_number']

X_train = X_train.reset_index(drop=True)
X_test = X_test.reset_index(drop=True)

scaler = StandardScaler()
scaler.fit(X_train[['room_number', 'Area','Index_value', 'Area_per_room', 'floor_ratio']])
X_train[['room_number', 'Area','Index_value','Area_per_room','floor_ratio']] = scaler.transform(X_train[['room_number', 'Area','Index_value','Area_per_room','floor_ratio']])
X_test[['room_number', 'Area','Index_value','Area_per_room','floor_ratio']] = scaler.transform(X_test[['room_number', 'Area','Index_value','Area_per_room','floor_ratio']])

corr = X_train.corr()
corr = corr.corr()

threshold = 0.67
high_corr_pairs = []
for i in range(len(corr.columns)):
    for j in range(i + 1, len(corr.columns)):
        if abs(corr.iloc[i, j]) > threshold:
            pair = (corr.columns[i], corr.columns[j])
            high_corr_pairs.append(pair)

for pair in high_corr_pairs:
    print(f"{pair[0]} and {pair[1]} have a high correlation: {corr.loc[pair[0], pair[1]]}")

independent_vars = X_train

vif = pd.DataFrame()
vif["Feature"] = independent_vars.columns
vif["VIF Score"] = [variance_inflation_factor(independent_vars.values, i) for i in range(independent_vars.shape[1])]

c_remove = ['condition _not_defined', 'furniture _not_defined', 'Area','type_Apartment','hasAirCondition ','hasMamad ','hasElevator ']
X_train = X_train.drop(c_remove, axis=1)
X_test = X_test.drop(c_remove, axis=1)

model = ElasticNet()
param_dist = {
     'alpha': uniform(loc=0.1, scale=10),
     'l1_ratio': uniform(loc=0.0, scale=1.0)}
random_search = RandomizedSearchCV(estimator=model, param_distributions=param_dist, n_iter=10, cv=5)
random_search.fit(X_train, y_train)
best_model = random_search.best_estimator_
y_pred = best_model.predict(X_test)
mse = mean_squared_error(y_test, y_pred)
rmse = sqrt(mse)
best_alpha = random_search.best_params_['alpha']
best_l1_ratio = random_search.best_params_['l1_ratio']

print("MSE:", mse)
print("RMSE:", rmse)

y_pred = best_model.predict(X_test)

print("Prediction:", y_pred)

joblib.dump(best_model, 'trained_model.pkl')

